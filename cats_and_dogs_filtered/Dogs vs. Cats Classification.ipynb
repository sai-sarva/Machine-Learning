{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The Asirra data set\n",
    "Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords.\n",
    "\n",
    "Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface:\n",
    "\n",
    "Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research.\n",
    "\n",
    "# Image recognition attacks\n",
    "While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random. There is enormous diversity in the photo database (a wide variety of backgrounds, angles, poses, lighting, etc.), making accurate automatic classification difficult. In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459.\n",
    "\n",
    "# State of the art\n",
    "The current literature suggests machine classifiers can score above 80% accuracy on this task [1]. Therfore, Asirra is no longer considered safe from attack.  We have created this contest to benchmark the latest computer vision and deep learning approaches to this problem. Can you crack the CAPTCHA? Can you improve the state of the art? Can you create lasting peace between cats and dogs?\n",
    "\n",
    "Okay, we'll settle for the former.\n",
    "\n",
    "# Acknowledgements\n",
    "We extend our thanks to Microsoft Research for providing the data for this competition.\n",
    "\n",
    "# Dataset Description\n",
    "The training archive contains 25,000 images of dogs and cats. Train your algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-30T09:06:52.375805900Z",
     "start_time": "2023-06-30T09:06:52.362497900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import os\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "set_config('diagram')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu:\n",
    "    print('GPU Available {}'.format(gpu))\n",
    "else:\n",
    "    print('No GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T08:45:00.339779700Z",
     "start_time": "2023-06-30T08:45:00.321093600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://storage.googleapis.com/tensorflow-1-public/course2/cats_and_dogs_filtered.zip\"\n",
    "response = requests.get(url, stream=True)\n",
    "filename = os.path.join(os.getcwd(), \"cats_and_dogs_filtered.zip\")\n",
    "\n",
    "with open(filename, 'wb') as fd:\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            fd.write(chunk)\n",
    "\n",
    "print(\"Download complete.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T08:45:12.252516900Z",
     "start_time": "2023-06-30T08:45:00.536648300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Unzipping the file\n",
    "with zipfile.ZipFile('cats_and_dogs_filtered.zip') as file:\n",
    "    file.extractall()\n",
    "    print('Extraction Success!!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T08:46:35.784255100Z",
     "start_time": "2023-06-30T08:46:33.472455500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of items in the training directory cats_and_dogs_filtered/train/, is 2\n",
      "The number of items in the test directory cats_and_dogs_filtered/validation/, is 2\n"
     ]
    }
   ],
   "source": [
    "current_working_direc = os.getcwd()\n",
    "\n",
    "train_data_path = 'cats_and_dogs_filtered/train/'\n",
    "test_data_path = 'cats_and_dogs_filtered/validation/'\n",
    "\n",
    "def count_items_direc(path):\n",
    "    return len(os.listdir(path))\n",
    "\n",
    "print(f'The number of items in the training directory {train_data_path}, is {count_items_direc(train_data_path)}')\n",
    "print(f'The number of items in the test directory {test_data_path}, is {count_items_direc(test_data_path)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T08:48:48.811085Z",
     "start_time": "2023-06-30T08:48:48.799068800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "optimizer = 'RMSprop'\n",
    "class_mode = 'binary'\n",
    "epochs = 100\n",
    "loss = 'binary_crossentropy'\n",
    "image_size = 150"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T09:23:05.443151900Z",
     "start_time": "2023-06-30T09:23:05.420138300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initialize ImageDataGenerator instances for training and testing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.25,\n",
    "    rotation_range=45,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_data_gen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.25,\n",
    "    rotation_range=45,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Generator for training data\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_path,\n",
    "    target_size=(image_size, image_size),\n",
    "    class_mode=class_mode\n",
    ")\n",
    "\n",
    "# Generator for test data\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_path,\n",
    "    target_size=(image_size, image_size),\n",
    "    class_mode=class_mode\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T09:23:05.796485200Z",
     "start_time": "2023-06-30T09:23:05.652443500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 148, 148, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 74, 74, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 43, 43, 32)        524320    \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 21, 21, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 19, 19, 64)        18496     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 23104)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 23105     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 566,369\n",
      "Trainable params: 566,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(input_shape=(image_size, image_size, 3), filters=16, activation='relu', padding='valid',\n",
    "                               kernel_size=(3, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(32, 32), padding='valid', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='valid', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T09:23:05.919888Z",
     "start_time": "2023-06-30T09:23:05.852212900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(restore_best_weights=True, patience=10, monitor='val_loss', verbose=1)\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'logs/fit/', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "\n",
    "model_checkpoint_path = 'model_checkpoints/'\n",
    "if not os.path.exists(model_checkpoint_path):\n",
    "    os.mkdir(model_checkpoint_path)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath=model_checkpoint_path+'model-{epoch:02d}.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T09:23:06.053120200Z",
     "start_time": "2023-06-30T09:23:06.021614400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\sayan\\\\PycharmProjects\\\\Machine Learning\\\\logs/fit/20230630-192306'"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T09:23:06.321525Z",
     "start_time": "2023-06-30T09:23:06.298280100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 30s 473ms/step - loss: 1.1653 - accuracy: 0.4950 - val_loss: 0.6932 - val_accuracy: 0.5160\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 17s 263ms/step - loss: 0.6939 - accuracy: 0.4800 - val_loss: 0.6923 - val_accuracy: 0.5190\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 17s 270ms/step - loss: 0.7000 - accuracy: 0.4825 - val_loss: 0.6913 - val_accuracy: 0.5190\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.7025 - accuracy: 0.5060 - val_loss: 0.6884 - val_accuracy: 0.5500\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.7561 - accuracy: 0.5360 - val_loss: 0.6729 - val_accuracy: 0.5970\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 17s 264ms/step - loss: 0.6896 - accuracy: 0.5665 - val_loss: 0.6852 - val_accuracy: 0.5410\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.6613 - accuracy: 0.6015 - val_loss: 0.6653 - val_accuracy: 0.6130\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.6458 - accuracy: 0.6510 - val_loss: 0.6269 - val_accuracy: 0.6600\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 16s 261ms/step - loss: 0.6171 - accuracy: 0.6610 - val_loss: 0.6163 - val_accuracy: 0.6530\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 16s 250ms/step - loss: 0.6218 - accuracy: 0.6670 - val_loss: 0.5946 - val_accuracy: 0.6740\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 16s 262ms/step - loss: 0.6068 - accuracy: 0.6740 - val_loss: 0.6687 - val_accuracy: 0.6470\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 17s 263ms/step - loss: 0.5997 - accuracy: 0.6800 - val_loss: 0.5800 - val_accuracy: 0.6970\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 16s 258ms/step - loss: 0.5853 - accuracy: 0.6915 - val_loss: 0.5848 - val_accuracy: 0.6960\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 16s 262ms/step - loss: 0.5972 - accuracy: 0.6855 - val_loss: 0.6115 - val_accuracy: 0.6700\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 17s 264ms/step - loss: 0.5716 - accuracy: 0.6990 - val_loss: 0.6476 - val_accuracy: 0.6010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 16s 251ms/step - loss: 0.5799 - accuracy: 0.7040 - val_loss: 0.5895 - val_accuracy: 0.6880\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 16s 261ms/step - loss: 0.5695 - accuracy: 0.6980 - val_loss: 0.6473 - val_accuracy: 0.6690\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 16s 261ms/step - loss: 0.5723 - accuracy: 0.7130 - val_loss: 0.5976 - val_accuracy: 0.7030\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.5763 - accuracy: 0.7010 - val_loss: 0.6004 - val_accuracy: 0.6740\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 17s 263ms/step - loss: 0.5609 - accuracy: 0.7195 - val_loss: 0.6381 - val_accuracy: 0.6400\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 16s 258ms/step - loss: 0.5533 - accuracy: 0.7225 - val_loss: 0.6250 - val_accuracy: 0.6870\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5649 - accuracy: 0.7100Restoring model weights from the end of the best epoch: 12.\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.5649 - accuracy: 0.7100 - val_loss: 0.5905 - val_accuracy: 0.7030\n",
      "Epoch 22: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[tensorboard_callback,checkpoint_callback,early_stop]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T09:30:23.871579800Z",
     "start_time": "2023-06-30T09:24:10.339132400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
